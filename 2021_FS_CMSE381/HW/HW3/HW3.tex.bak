% Don't touch this %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage[left=1in,top=1in,right=1in,bottom=1in,headheight=3ex,headsep=3ex]{geometry}
\usepackage{graphicx}
\usepackage{float}


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{question}[theorem]{Question}
\newtheorem{answer}[theorem]{Answer}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{example}[theorem]{Example}
\newenvironment{proof}{\noindent \textbf{Proof:}}{$\Box$}


\newcommand{\blankline}{\quad\pagebreak[2]}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Modify Course title, instructor name, semester here %%%%%%%%

\title{\underline{CMSE 381: HW3 }}
\date{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Don't touch this %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[sc]{mathpazo}
\linespread{1.05} % Palatino needs more leading (space between lines)
\usepackage[T1]{fontenc}
\usepackage[mmddyyyy]{datetime}% http://ctan.org/pkg/datetime
\usepackage{advdate}% http://ctan.org/pkg/advdate
\newdateformat{syldate}{\twodigit{\THEMONTH}/\twodigit{\THEDAY}}
\newsavebox{\MONDAY}\savebox{\MONDAY}{Mon}% Mon
\newcommand{\week}[2]{%
%  \cleardate{mydate}% Clear date
% \newdate{mydate}{\the\day}{\the\month}{\the\year}% Store date
  \paragraph*{\kern-2ex\quad #1, \syldate{\today} - \AdvanceDate[4]\syldate{\today}:}% Set heading  \quad #1
%  \setbox1=\hbox{\shortdayofweekname{\getdateday{mydate}}{\getdatemonth{mydate}}{\getdateyear{mydate}}}%
  \ifdim\wd1=\wd\MONDAY
    \AdvanceDate[7]
  \else
    \AdvanceDate[7]
  \fi%
}
\usepackage{setspace}
\usepackage{multicol}
%\usepackage{indentfirst}
\usepackage{fancyhdr,lastpage}
\usepackage{url}
\pagestyle{fancy}
\usepackage{hyperref}
\usepackage{lastpage}
\usepackage{amsmath}
\usepackage{layout}

\lhead{}
\chead{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Don't touch this %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\lfoot{}
\cfoot{\small \thepage/\pageref*{LastPage}}
\rfoot{}

\usepackage{array, xcolor}
\usepackage{color,hyperref}
\definecolor{clemsonorange}{HTML}{EA6A20}
\hypersetup{colorlinks,breaklinks,linkcolor=clemsonorange,urlcolor=clemsonorange,anchorcolor=clemsonorange,citecolor=black}
\usepackage{txfonts}
\begin{document}

\maketitle

\blankline

\begin{enumerate}
	\item[1] (10 pts) Exercise 3.7.3
	\item[2] (8 pts) Exercise 3.7.4
	\item[3] (10 pts) Exercise 3.7.9 (Don't do (d)). Please read through chapter 3.6 before doing those applied questions.  
	\item[4] (12 pts) Exercise 3.7.10 (a, b, c, d, e, f)
	\item[5] (10 pts) Exercise 3.7.13
	\item[6] (10 pts) Exercise 3.7.14 (a, b,c, d, e, f)
	\item[7] (20 pts)  We assume $Y = X^T \beta + \epsilon$ with $\epsilon \sim N(0, \sigma^2)$. We collect a set of training data $\{(x_1, y_1), \ldots, (x_n, y_n) \}$  and let $\mathbf{Y} = (y_1, \ldots, y_n)^T$ and $\mathbf{X} = (x_1, \ldots, x_n)^T$, where $x_i = (1, x_{i1}, \ldots, x_{i p})^T$. We want to fit a multiple linear regression.
	\begin{itemize}
	\item[a.] Derive the $\hat{\boldsymbol{\beta}}$ minimizing the RSS, where $\hat{\boldsymbol{\beta}} = (\hat{\beta}_0, \ldots, \hat{\beta}_p)^T$.
	\item[b.] Prove that this $\hat{\boldsymbol{\beta}}$ is an unbias estimator for the true $\boldsymbol{\beta}$.
	\item[c.] Derive $\text{Var}(\hat{\boldsymbol{\beta}})$. 
	\item[d.] Let $p = 1$, and using (c.) to prove the equation (3.8). Hint: you may need the formula for the inverse of a 2x2 matrix. 
	\item[e.] Prove that $\boldsymbol{\epsilon} = \mathbf{Y} - \mathbf{X} \hat{\boldsymbol{\beta}}$ is orthogonal to the column space of $\mathbf{X}$. Namely,
	\[
     \boldsymbol{\epsilon}^T \mathbf{X} = \mathbf{0}.
	\]	
	\end{itemize}
	\item[8] (20 pts) We will now use simulation to test the confidence interval and prediction interval. We assume the data are generated according to 
	\[
		Y = \beta_0 + X \beta_1 + \epsilon, \ \ \epsilon \sim N(0, 0.2^2).
	\] 
	You can using the following code to simulate a training set of 100 data with fixed $X = (x_1, \ldots, x_{100})^T$, a new response variable $Y$ and the expection of $Y$ for $X = 1$.
	\ \ set.seed(36)  \\
	\ \ $X = rnorm(100)$  \# we will fix this X \\
	\ \ $ Y = 1 + 0.2 * X + 0.2 * rnorm(100)$ \\
    \ \ $Y.new = 1 + 0.2 * 1 + 0.2 * rnorm(1)$ \\
    \ \ $F.X = 1 + 0.2 * 1$ \# this is expectation of $Y$ when $X = 1$ \\
   \begin{itemize}
   \item[a][2 pts]  Using this training data to get the $90\%$ confidence interval and prediction interval for $X = 1$, and check whether the confidence interval contains $F.X$ and whether the prediction interval contains $Y.new$.
   \item[b] [10 pts] Repeat the procedure 2000 times for generating training data, $Y.new$, $F.X$, $90\%$ confidence interval and prediction interval. Report how many time the confidence interval contains $F.X$ and how many time the prediction interval contains $Y.new$.
   \item[c][8 pts] Using the same setting to generate 50 different training datasets, and generate two plot similar to Fig 3.3 in the textbook.  
\end{itemize}    
   \item[9] (Challenging problem, not required) Another important property for the least squares estimates (the one in Q7, and we denote $\hat{\boldsymbol{\beta}}$ as $\hat{\boldsymbol{\beta}}^{\text{ols}}$) is stated in the Gauss-Markov Theorem.  We first introduce some concepts:
\begin{itemize}
	\item An estimator that is a linear function of $\mathbf{Y} = (y_1, \ldots, y_n)^T \in \mathbb{R}^n$ is said to be a {\bf linear estimator}. 
	\item A linear estimator $c^T \mathbf{Y}$ is an unbiased estimator of $a^T \boldsymbol{beta} $ if and only if $E (c^T \mathbf{Y}) = a^T \boldsymbol{\beta}$ for $\forall \boldsymbol{\beta} \in \mathbb{R}^p$.
\end{itemize}
\begin{theorem}[Gauss-Markov] Based on the linear model assumption $y = x^T \boldsymbol{beta}^* + \epsilon$,  among all the linear unbiased estimators for $a^T \boldsymbol{\beta}^*$  ( for example $\tilde{\boldsymbol{\theta}} = c^T \mathbf{Y}$ linear in terms of  $\mathbf{Y}$), $a^T \hat{\boldsymbol{\beta}}^{ols}$ has the smallest variance. Namely, for $\forall c \in \mathbb{R}^n$ such that $E (c^T \mathbf{Y}) = a^T \boldsymbol{\beta}$, we have
	\[
	\text{Var}(a^T \hat{\boldsymbol{\beta}}^{ols}) \leq \text{Var}(c^T \mathbf{Y}).
	\]
\end{theorem}

Why is this important? Because it says that our predictions $x_0^T \hat{\boldsymbol{\beta}}$ at any $x_0 \in \mathbb{R}^p$ will be unbiased for the true mean $x_0^T \boldsymbol{\beta}^*$ at $x_0$. 
Thus, the Gauss-Markov Theorem ensures that among all the linear unbiased predictions, $x_0^T \hat{\boldsymbol{\beta}}^{\text{ols}}$ results the smallest variance. We also call $\hat{\boldsymbol{\beta}}_{\text{ols}}$ the Best Linear Unbiased Estimator (BLUE).  Prove this theorem. 


  

\end{enumerate}

\end{document}



